---
title: "EM Algorithm"
author: "T.SHIBUYA"
date: "2018/05/31"
output:
  html_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

###  EMアルゴリズム  

　EMアルゴリズム（Dempster et al. 1997; 宮川，1987）は欠測値や観測不可能な潜在変数を含む尤度関数を最大化（最小化）する計算手法の一種である。因子分析やIRT（項目反応理論）のパラメタ推定，自然言語処理のクラスタリングなどの様々な手法においてこの手法は用いられている。本稿ではこの計算アルゴリズムについて解説をおこなう。  

　観測されたデータをX，推定したいパラメタ（母数）を $\theta$ とおく。この尤度関数は，
$$
L(\theta |X)=p(X|\theta )
$$

とあらわすことができる。この式は母数$\theta$が与えられたときの，データ$X$の確率密度関数を表しており，尤度関数と確率密度関数は式の定義上では条件付けられている値が逆になっているが，計算上は全く同一である。  
　実際の計算では対数尤度関数を最大化する。完全データの対数尤度関数は基本的に計算しやすい。なお，対数化しても，対数関数は単調増加するため，もとの尤度関数と最大点（最尤推定値）は変わらないことが保証されている  

　しかし，この尤度関数に潜在変数(latent valiable, hidden variable)や欠測値(missing value)が入り込んでいる場合がある。こうしたデータを不完全データ(imcomplete data)と呼ぶが，つまり，
$$
L(X,Z|\theta )
$$
を計算することになる。一般に潜在変数や欠測値の混ざった対数尤度関数の計算は容易ではない。潜在変数や欠測値がデータに含まれる場合，その変数や欠測のパターンの数だけ異なる尤度関数が与えられる。その尤度関数ごとにパラメタで微分しなければならないが，その最尤推定値は明示的に与えられず，数値解析法が必要とされる。

  
　しかしポピュラーな数値解析法であるニュートンラフソン法は二階偏微分を要素にもつヘッセ行列の計算が必要となる。この逆行列の計算の負荷は大きく，ときに計算をおこなえない場合もある。

  
　そこで，適当な $\theta^{(t)}$ の値をもとに，さらに尤度を大きくするような $\theta^{(t+1)}$ を繰り返し求めていく，EMアルゴリズムとよばれる方法がもちいられる。

　EMアルゴリズムでは不完全データの対数尤度関数の代わりに代理関数（Q関数）を定義して，これを最大化する。　　
  
今，反復t回目のパラメタの推定値を$\theta^{(t)}$とすると，代理関数は

$$
\begin{align}
Q(\theta ^{(t+1)} |\theta^{(t)}) &= E \rm [\ln p(X,Z;\theta ^{(t+1)})  |  D;\theta^{(t)}]\\
                                 &= \int \ln p(X,Z|\theta^{(t+1)} )p(Z|X;\theta^{(t)})dZ\\
                                 &=  \Sigma_Z P(Z|X;\theta^{(t)}) \ln p(X,Z|\theta^{(t+1)})
\end{align}
$$
と定義することができる。この積分の対象になっている関数が完全データの期待対数尤度関数である。つまり，この関数は観測値$X$が得られたときの，潜在変数$Z$の条件つき確率分布についての期待値を取っている。この代理関数を最大化するような $\theta^{(t+1)}$ を求めれば， $\theta^{(t)}$ よりも尤度を大きくするようなパラメタを得ることができる。　　

  また，上式中の$P(Z|X;\theta^{(t)})$は観測値$X$が得られた条件下での$Z$の事後分布にあたる。これの計算はベイズの定理により，

$$
P(Z|X;\theta^{(t)}) = \frac{p(X,Z;\theta^{(t)})}{\Sigma_Z P(X,Z;\theta^{(t)})}
$$
で計算可能である。樺島・上田（2003）はこのQ関数の計算について「直感的には，現在のパラメータ推定値に基づいて算出される潜在変数の事後分布$P(Z|X;\theta^{(t)})$をその信頼度として不完全情報を暫定的に補っていると解釈できる」と表現している（式中の文字は，本稿に合うように筆者が一部変更した）。

このとき問題によっては潜在変数$Z$の事後分布を計算することが困難な事がある。そのような場合には事後分布を計算が容易な分布（例えば正規分布やベルヌーイ分布など）を代わりに導入し，Eステップの近似値を計算することが考えられる。これを一般化EM法と呼ぶ。  


###  Eステップ  

　Eステップでは潜在変数（欠測値）の生起確率を重みとして期待値を計算している。例えばこんなデータがあるとする。 $\theta$ は潜在変数である。

|受験者| $x_{j1}$ | $x_{j2}$ | $x_{j3}$ | $\theta$ |
|---|---|---|---|---|
|1| $x_{11}$ | $x_{12}$ | $x_{13}$ |?|  

　この潜在変数が，カテゴリカルなものであったり，あるいはほかの変数と同じような欠測値であった場合，その生起確率を重みとして，このような復元データの表が得られる。  

|受検者| $x_{j1}$ | $x_{j2}$ | $x_{j3}$ | $\theta$ |重み|
|---|---|---|---|---|---|
|1-1| $x_{11}$ | $x_{12}$ | $x_{13}$ | $\theta_1$ | $P(\theta_2)$ |
|1-2| $x_{11}$ | $x_{12}$ | $x_{13}$ | $\theta_2$ | $P(\theta_2)$ |
|1-3| $x_{11}$ | $x_{12}$ | $x_{13}$ | $\theta_3$ | $P(\theta_3)$ |
  

```{r  echo=FALSE, comment=NA, include=FALSE}
dat <- cbind(c(1,1,0),c(1,0,1),c(0,0,1),c("?","?","?"))
colnames(dat) <- c("x1","x2","x3","theta")
rownames(dat) <- c("sub1", "sub2", "sub3")
return(dat)

```


　このデータにおける生起確率は　$\theta^{(t)}$ をもちいて計算されるものである。もちろん $\theta$ の値が連続変数である場合は期待値を取る際に積分をおこなう必要があり，一般的に区分求積法や，より精度のよいガウス・エルミート求積法などで近似計算される。
  
　教科書p87の例で言えば，得られている観測値は文書ベクトルの集合であり，潜在変数がクラスタに属する確率，推定したいパラメタが各クラスタの平均ベクトルである。つまりEステップでは，各文書ベクトルについて「全部のクラスタに属する確率を重みとして」，期待値をとっているのである。それが教科書p88の式3.7
$$
\Sigma_{x^{(i)}\in D}\Sigma_c P(c|x^{(i)};\theta)+logP(c,x^{(i)};\theta)
$$
で実行される計算である。  
　このようにEステップでは条件つき確率をもとに期待値を計算し，さらにその期待値を潜在変数に代入して計算をおこなっている。  


###  Mステップ  
　Mステップで扱う代理関数には，もはや当初のような扱いにくさはない。通常の最尤推定法と同様にパラメタについて最大化すればよい。  
　
　このMステップでの最大化によって得られたパラメタが $\theta^{(t+1)}$ であり， $\theta^{(t)}$ との差が一定量まで小さくなった段階で反復計算を終了すればよい。  
　授業内のテキストではこの期待対数完全データ尤度関数を，パラメタで偏微分して＝0とおいた方程式を解くだけで最大化が可能であった。しかし，例えばIRTの項目パラメタ推定法においては，この期待完全データ対数尤度関数をニュートンラフソン法やフィッシャーのスコア法で最大化する方法が一般的に用いられている（Bock & Aitkin, 1981; 加藤ら，2014）。この場合の問題点として，これらの数値計算での最大化が失敗した場合に，必ずしも $\theta^{(t+1)}$ が $\theta^{(t)}$ よりも大きな尤度を与えるパラメタとならないことが指摘されている（高井ら，2016）。
  
###  Q関数の導出  

　完全データ対数尤度関数は，観測値の対数尤度関数と潜在変数の尤度関数の和で表現される。すなわち，  
　
$$
\ln p(\rm X,Z|\theta) = \ln \it p(\rm X|\theta) + \ln \it p(\rm{Z}|X,\theta)  
$$
$$
\ln \it p(\rm X|\theta) = \ln p(\rm X,Z|\theta) - \ln \it p(\rm{Z}|X,\theta)  
$$

ここで両辺を $X$ と $\theta^{(t)}$ に関して期待値を取れば，  

（期待値を取るときはパラメタを標記しないが，EMアルゴリズムでの計算に関しては真ではなく暫定的なパラメタを用いていることから，強調してパラメタが標記されることが多い。そのため「；」で $\theta^{(t)}$ は記している。）
$$
E[\ln \it p\rm (X|\theta)|X;\theta^{(t)}] = E[\ln \it p \rm (X,Z|\theta)|X;\theta^{(t)}] - E[\ln \it p(\rm{Z}|X,\theta)|X;\theta^{(t)}]  
$$

となる。このとき左辺の期待値は結局のところ $\ln \it f(\rm X|\theta)$ を計算していることに等しい。したがってこのまま右辺を最大化することが，本来目的であった完全データ対数尤度関数の最大化につながる。  
　右辺の第一項は，EステップのところでみたQ関数に等しい。第二項はH関数と呼ばれ，
　
$$
H(\theta|\theta^{(t)})=E[\ln \it p(\rm{Z}|X;\theta)|X;\theta^{(t)}]  
$$
とおいておく。このH関数は対数尤度の差を計算して尤度が増加していることを確認する際に無視できるので通常問題にされることはない。　　
（詳細は高井ら「欠測データの統計化学」p83などを参照のこと。）　　


###  EMアルゴリズムの特徴　　


　EMアルゴリズムはQ関数の最大化によりパラメタの推定値を更新していく計算アルゴリズムである。この最大化されたパラメタはひとつまえのパラメタよりも必ず大きな尤度を与える値となっている。したがってEMアルゴリズムは最大化が成功していれば単調増加静を持っており，尤度関数がどこかで最大となる点を持っているのであれば必ず収束する。  
　
しかし，EMアルゴリズムを用いたとしても，局所的な解に落ちてしまうこともある。収束先が局所的な解に落ちてしまっていないかを判断するためには，複数の初期値を与えて収束先の尤度を比較するしかない。
  
  
###  EMアルゴリズムがなぜ最大化（最小化）する母数を推定可能なのか  

　いま，t回目の反復によって得られた推定値$\theta^{(t)}$が得られているとする。その状況での$Z$の事後分布を$P(Z|X;\theta^{(t)})$とし，これをデータと潜在変数の尤度関数$p(X,Z;\theta^{(t)})$に対して*乗算，除算する*。すると，
$$
\ln{p(X;\theta)}=\ln{\Sigma_Z {P(Z|D;\theta^{(t)})\frac{p(X,Z;\theta)}{P(Z|X;\theta^{(t)})}}}
$$
という式が得られる。式中の右辺は$\frac{p(X,Z;\theta^{(t)})}{P(Z|X;\theta^{(t)})}$を事後分布で期待値をとった式とみなすことができる（シグマに注目）。すなわち期待値を$E$で置き換えると，
$$
\ln{p(X;\theta)}=E_Z\left[\frac{p(X,Z;\theta)}{P(Z|X;\theta^{(t)})}\middle | X,\theta^{(t)} \right]
$$

のように表すことができる。
  ここで対数関数に関するJensen（イェンセン）の不等式を利用する。Jensen不等式は上に凸な関数において，
$$
f(E(x)) \geq E(f(x))
$$
  
という関係を保証するものである。厳密な証明には数学的帰納法などが用いられるが，直感的な理解としては以下のグラフで説明ができるだろう。
```{r}

```
$f(x)=-x^2$という単純な関数を考え，理解のために，$1,2,3$の3つの離散点を用いる。不等式の左辺はxについての期待値を関数に与えた値なので，$f((1^2+2^2+3^3)/3)=7^2=49$となるが，不等式の右辺は関数値の期待値をとるものなので，$(f(1)+f(2)+f(3))/3=(1+4+9)/3=4.667$となり，ごく部分的な例ではあるが不等式が成り立つ事が分かる。この関係は対数においても同様であり，

$$
\ln f(E(x)) \geq E(\ln f(x))
$$
も成り立つ。  


　この不等式を用いると，先に説明した事後確率分布で期待値をとった式を利用してデータの事後分布は以下のような不等式で表すことができる。
$$
\begin{align}
\ln{p(X;\theta)} &= \ln{E_Z\left[\frac{p(X,Z;\theta)}{P(Z|X;\theta^{(t)})}\middle | X,\theta^{(t)} \right]}\\
                 &\geq E_Z{\ln\left[\frac{p(X,Z;\theta)}{P(Z|X;\theta^{(t)})}\middle | X,\theta^{(t)}\right]}\\
                 &= \Sigma_Z P(Z|X;\theta^{(t)})\ln{\frac{p(X,Z;\theta)}{P(Z|X;\theta{(t)})}}
\end{align}
$$

注）2行目でJensen不等式を当てはめたため，対数記号$\ln$の中の期待値記号$E$が外に出てくる。そして3行目では$Z$について$D$と$\theta^{(t)}$が所与の条件における期待値をとっているため，$\Sigma$記号に置き換わっている。

これによって対数尤度$\ln{p(X;\theta)}$の下限値が分かった。

また，ここでベイズの定理より$P(Z|X;\theta^{(t)})$は，データと欠測値（潜在変数）の同時確率をもちいて
$$
P(Z|X;\theta^{(t)})=\frac{p(D,Z;\theta^{(t)})}{\Sigma_Zp(D,Z;\theta^{(t)})}
$$
により計算される。  

　ここで，$f(\theta|\theta^{(t)})= \Sigma_Z P(Z|X;\theta^{(t)})\ln{\frac{p(X,Z;\theta)}{P(Z|X;\theta{(t)})}}$(対数尤度関数の下限値)とおき，$\ln p(X;\theta)-f(\theta|\theta^{(t)})$を計算する。このとき，異なる確率分布間の差（距離）を表す指標であるKLダーバージェンス，
$$
KL(P,Q)=\Sigma_XP(X)\ln\frac{P(X)}{Q(X)}
$$
を用いて，
$$
\begin{align}
\ln{p(X;\theta)} - f(\theta|\theta^{(t)}) &= KL(P(\cdot|X;\theta^{(t)}),P(\cdot|X;\theta)\\
\ln{p(X;\theta)}                          &= f(\theta|\theta^{(t)})+KL(P(\cdot|X;\theta^{(t)}),P(\cdot|D;\theta)\\
\end{align}
$$

ここでさらに，上式で$\theta=\theta^{(t)}$とおいた$ln{p(X;\theta^{(t)})}$を引いてやると，
$$
\ln{p(X;\theta)} - \ln{p(X;\theta^{(t)})} = f(\theta|\theta^{(t)}) - f(\theta^{(t)}|\theta^{(t)}) + KL(P(\cdot|X;\theta^{(t)}),P(\cdot|X;\theta)
$$

注）$\ln{p(X;\theta^{(t)}$式の展開後の右辺のKL項は，`log(1)=0`により消える。

となる。KL項はJensen不等式により非負であることが分かっているので，$ f(\theta|\theta^{(t)}) \geq f(\theta^{(t)}|\theta^{(t)})$であるとき$\ln{p(X;\theta)} \geq \ln{p(X;\theta^{(t)})}$である。



###  変分ベイズ法

　一般化EM法ではEステップの計算が困難な問題に対して，適当な分布で近似計算をおこなった。これから説明する変分ベイズ（Variational Bays, VB）法は，一般化EM法をベイズ推定へ発展させた方法である。VBはマルコフ連鎖モンテカルロ法（MCMC）のような計算機パワーを駆使する近似法とは異なり，効率よく近似計算をおこなうことができる手法である。  
　
ベイズ推定では事前分布と得られたデータの情報（尤度関数）を利用して事後分布を定式化する方法であり，その事後分布の直接的な計算は容易でないため，乱数による近似計算などで計算されている。VB法では一般化EM法と同様に事後分布をベイズの定理のそとからもってきて近似的に求める手法である。  




  
####  引用文献
教科書　＝　高村大也 (2010). 言語処理のための機械学習入門　コロナ社
Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Maximum likelihood from incomplete data via the EM algorithm. Journal of Royal Statistical Society. Series B(Methodological), 39, 1-38.


